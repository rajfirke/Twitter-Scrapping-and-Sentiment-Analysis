# -*- coding: utf-8 -*-
"""Twitter Scrapping and Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hlfBRWWwF2VW1TEHv7tVHMYgMwBP1rQE

# Dependancies
"""

pip install ntscraper

import pandas as pd
from ntscraper import Nitter

import nltk
nltk.download('vader_lexicon')

from nltk.sentiment import SentimentIntensityAnalyzer

scrapper = Nitter()

sid = SentimentIntensityAnalyzer()

"""# Scrapping"""

tweets = scrapper.get_tweets('imVkohli', mode= 'user', number = 1000)

tweets

for tweet in tweets['tweets']:
  print(tweet)

final_tweets = []
for x in tweets['tweets']:
    data = [x['link'], x['text'],x['date'],x['stats']['likes'],x['stats']['comments']]
    final_tweets.append(data)

"""# Pre-Processing"""

df = pd.DataFrame(final_tweets, columns =['twitter_link','text','date','likes','comments'])

df

df['date'] = pd.to_datetime(df['date'], format="%b %d, %Y Â· %I:%M %p UTC")

df = df.sort_values(by='date')

df

df.isnull().sum()

"""# Sentiment Analysis"""

df['compound'] = df['text'].apply(lambda x: sid.polarity_scores(x)['compound'])

df['sentiment'] = df['compound'].apply(lambda x: 'pos' if x >= 0.05 else ('neg' if x <= -0.05 else 'neu'))

print(df)

sentiment_counts = df['sentiment'].value_counts()
print("\nSentiment Distribution:")
print(sentiment_counts)

average_compound_score = df['compound'].mean()
print("\nAverage Compound Score:", average_compound_score)

df.to_csv('File_name.csv', index=False)